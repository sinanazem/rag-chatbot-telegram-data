{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb82ba52-3df5-443f-9bf6-0ebb04b95d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list, Stemmer\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import arabic_reshaper\n",
    "import demoji\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from bidi.algorithm import get_display\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2202f3a-fcb4-40da-bd1a-977c8a17a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"pytopia\"\n",
    "COLLECTION_NAME = \"messages\"\n",
    "START_DATE = '2022-07-01T00:00:00'\n",
    "END_DATE = '2023-07-31T23:59:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dedce3eb-124a-46b2-ad79-515603e58abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client[DB_NAME]\n",
    "cursor = db[COLLECTION_NAME].find({})\n",
    "# for document in cursor:\n",
    "#     pprint(document)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72748df2-7dac-4f36-91f4-25f45b596dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data = [doc for doc in cursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ea7897-dd14-4061-8e33-b34f5cc56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4154f8f3-228e-4633-ba5b-8fa7eba0bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-10 16:05:24.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mLoading stopwords from stopwords.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# load stopwords\n",
    "logger.info(f\"Loading stopwords from {'stopwords.txt'}\")\n",
    "stop_words = open('stopwords.txt').readlines()\n",
    "stop_words = map(str.strip, stop_words)\n",
    "stop_words = set(map(normalizer.normalize, stop_words))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(normalizer.normalize(text))\n",
    "    tokens = list(filter(lambda item: item not in stop_words, tokens))\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def de_emojify(text):\n",
    "    regrex_pattern = re.compile(pattern=\"[\\u2069\\u2066]+\", flags=re.UNICODE)\n",
    "    text = regrex_pattern.sub('', text)\n",
    "    return demoji.replace(text, \" \")\n",
    "\n",
    "def preprocess_text(msg):\n",
    "    text_content = ''\n",
    "    if isinstance(msg, list):\n",
    "        for sub_msg in msg:\n",
    "            if isinstance(sub_msg, str):\n",
    "                text_content += f\" {remove_stopwords(sub_msg)}\"\n",
    "            elif isinstance(sub_msg, dict) and sub_msg['type'] in {\n",
    "                'text_link', 'bold', 'italic',\n",
    "                'hashtag', 'mention', 'pre'\n",
    "            }:\n",
    "                text_ = remove_stopwords(sub_msg['text'])\n",
    "                text_content += f\" {text_}\"\n",
    "    else:\n",
    "        text_content += f\" {remove_stopwords(msg)}\"\n",
    "\n",
    "    tokens = list(word_tokenize(normalizer.normalize(text_content)))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33f05cee-191b-4acb-bc08-b020eed9118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_docs):\n",
    "    docs = copy.deepcopy(input_docs)\n",
    "    for doc in docs:\n",
    "        if not doc.get('text'):\n",
    "            continue\n",
    "        content = doc['text']\n",
    "        doc['text'] = preprocess_text(content)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85e79e30-c4c0-4f3d-a221-0cf2d5c066f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ğŸ›‘Ø¨Ú†Ù‡ Ù‡Ø§ ÛŒÙ‡ Ù†Ú©ØªÙ‡ Ú©Ù‡ Ø§Ù„Ø¨ØªÙ‡ Ø®ÛŒÙ„ÛŒ Ù‡Ù… Ù…Ù‡Ù… Ù†ÛŒØ³Øª \\nØ´Ù…Ø§ ÙˆÙ‚ØªÛŒ Ø¨Ù‡ ØµÙˆØ±Øª manually   ÙØ§ÛŒÙ„ Ø±Ùˆ Ø§Ø² Ø§ÛŒÙ† Ù„ÛŒÙ†Ú© Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒ Ú©Ù†ÛŒØ¯\\n',\n",
       " {'type': 'link',\n",
       "  'text': 'https://docs.microsoft.com/en-us/windows/wsl/install-manual'},\n",
       " '\\nØ·Ø¨ÛŒØ¹ØªØ§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÛŒÚ© ÙØ§ÛŒÙ„ ØªÚ© Ø¨Ø§ Ù¾Ø³ÙˆÙ†Ø¯ appx Ø±Ùˆ Ø¨Ù‡ Ø±Ùˆ Ø¨Ø§Ø´ÛŒØ¯\\nØ­Ø§Ù„Ø§ Ø¨Ø¹Ø¶ÛŒ Ø¨Ú†Ù‡ Ù‡Ø§ Ú©Ù‡ Ø¨Ø§ IDM Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒ Ú©Ù†Ù† IDM Ù¾Ø³ÙˆÙ†Ø¯ ÙØ§ÛŒÙ„ Ø±Ùˆ Ø¨Ù‡ ZIP ØªØºÛŒÛŒØ± Ù…ÛŒØ¯Ù‡  Ùˆ Ø¨Ú†Ù‡ Ù‡Ø§ Ù‡Ù… Ø§ÙˆÙ† ÙØ§ÛŒÙ„ Ø±Ùˆ Ø¨Ø§Ø² Ù…ÛŒ Ú©Ù†Ù† Ùˆ Ø³Ø±Ø¯Ø±Ú¯Ù…  Ù…ÛŒØ´Ù†\\nÚ©Ù‡ Ù…Ø³Ø§Ù„Ù‡ Ø®Ø§ØµÛŒ Ù†ÛŒØ³Øª Ø®ÙˆØ¯ØªÙˆÙ† Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø³ÙˆÙ†Ø¯ ÙØ§ÛŒÙ„ Ø±Ùˆ Ø¯Ø³ØªÛŒ Ø§Ø² zip Ø¨Ú©Ù†ÛŒØ¯ appx  Ùˆ wsl Ø§ØªÙˆÙ† Ú©Ù‡ Ø§Ù„Ø§Ù† ÛŒÙ‡ ØªÚ© ÙØ§ÛŒÙ„ Ù‡Ø³Øª Ø±Ùˆ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[180][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdcbd5cb-a600-46ab-90c5-3c201f534a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "527fe6cb-1466-4de0-8ef4-653029f40c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('6661636c9aff4f6795898336'),\n",
       " 'id': 1041,\n",
       " 'type': 'message',\n",
       " 'date': '2021-07-06T12:35:21',\n",
       " 'date_unixtime': '1625562321',\n",
       " 'from': 'MEHDI',\n",
       " 'from_id': 'user310785297',\n",
       " 'reply_to_message_id': 629,\n",
       " 'text': ['Ø§Ø±Ù‡',\n",
       "  'Ù…ÛŒØ¯ÙˆÙ†Ù…',\n",
       "  'ÛŒÙ‡Ùˆ',\n",
       "  'Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ',\n",
       "  'ØªØµÙ…ÛŒÙ…',\n",
       "  'Ú¯Ø±ÙØªÙ…',\n",
       "  'Ø³ÙˆÛŒÛŒÚ†',\n",
       "  'Ø§Ø¨ÙˆÙ†ØªÙˆ',\n",
       "  'ğŸ˜‚'],\n",
       " 'text_entities': [{'type': 'plain',\n",
       "   'text': 'Ø§Ø±Ù‡ Ù…ÛŒØ¯ÙˆÙ†Ù… Ù…ÛŒØ´Ù‡ ÙˆÙ„ÛŒ Ù…Ù† ÛŒÙ‡Ùˆ Ø®ÛŒÙ„ÛŒ Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØªÙ… Ø³ÙˆÛŒÛŒÚ† Ú©Ù†Ù… Ø¨Ù‡ Ø§Ø¨ÙˆÙ†ØªÙˆ ğŸ˜‚'}]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a13c6423-6b80-488e-8a35-82602bdb42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_dict(docs):\n",
    "    \n",
    "    index = {}\n",
    "    for doc in docs:    \n",
    "        for pos, token in enumerate(doc['text']):\n",
    "            if token not in index:\n",
    "                index[token] = {\n",
    "                'total_freq': 1,\n",
    "                 'docs': {\n",
    "                       doc['id']: {\n",
    "                           'count_of_word': 1,\n",
    "                           'positions': [pos]\n",
    "                           }\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                index[token]['total_freq'] += 1  \n",
    "                if doc['id'] not in index[token]['docs']:\n",
    "                    index[token]['docs'][doc['id']] = {}\n",
    "                    index[token]['docs'][doc['id']]['count_of_word'] = 1\n",
    "                    index[token]['docs'][doc['id']]['positions'] = [pos]\n",
    "                else:\n",
    "                    index[token]['docs'][doc['id']]['count_of_word'] += 1 \n",
    "                    index[token]['docs'][doc['id']]['positions'].append(pos)   \n",
    "    \n",
    "    # create champion lists             \n",
    "    for word in index:\n",
    "        champ_list = sorted(index[word]['docs'], key=lambda x: index[word]['docs'][x]['count_of_word'], reverse=True)\n",
    "        index[word]['champions'] = champ_list[:len(champ_list) // 5]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2da43dd-e562-46bd-b49c-4f7b5c1f8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_docs = p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a99152e0-7b8e-4a71-8007-0ee03f0d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index_dict(p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d8a48aa-2e23-4768-b2a4-42f831f08004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_process(docs, tokens):\n",
    "    docs_score = {}\n",
    "    for token in tokens:\n",
    "        if token in index:\n",
    "            for doc_id in index[token]['docs']:\n",
    "                    if doc_id not in docs_score.keys():\n",
    "                        docs_score[doc_id] = 1\n",
    "                    else:\n",
    "                        docs_score[doc_id] += 1\n",
    "   \n",
    "    docs_score = sorted(docs_score.items(), key=lambda doc_score: doc_score[1], reverse=True)\n",
    "    result = [x for x,_ in docs_score]\n",
    "    return result\n",
    "\n",
    "\n",
    "def exceptions_process(docs, tokens):\n",
    "    removal_docs = []\n",
    "    for token in tokens:\n",
    "        rmv_docIds = index[token]['docs'].keys()\n",
    "        for doc_id in rmv_docIds:  \n",
    "             if removal_docs.count(doc_id) == 0:\n",
    "                    removal_docs.append(doc_id)\n",
    "\n",
    "    result = []            \n",
    "    for doc_id in docs:\n",
    "        if removal_docs.count(doc_id) == 0:\n",
    "            result.append(doc_id)\n",
    "\n",
    "    return result\n",
    "\n",
    "def doc_contain_phrase(docId, phrase):\n",
    "   # we need just a sequence of numbers in positions to find a doc contain special phrase\",\n",
    "    tokens_lst = phrase.split()\n",
    "    positions = []\n",
    "    for token in tokens_lst:\n",
    "        positions.append(list(index[token]['docs'][docId]['positions']))\n",
    "\n",
    "    for i in range (len(positions[0])):\n",
    "        flag = True\n",
    "        index_of_first_token = positions[0][i]\n",
    "        for j in range (len(positions)):\n",
    "            if positions[j].count(index_of_first_token + j) == 0:\n",
    "                flag = False\n",
    "        if flag :\n",
    "            return True\n",
    "    return flag\n",
    "\n",
    "def phrasal_process(docs, phrases):\n",
    "    posting_lists = []\n",
    "    result = []\n",
    "    for phrase in phrases:\n",
    "        words = phrase.split()\n",
    "        for word in words:\n",
    "            if word in index:\n",
    "                posting_lists.append(list(index[word]['docs'].keys()))\n",
    "    \n",
    "        intersection_doc_ids = set.intersection(*map(set, posting_lists))\n",
    "\n",
    "        for docId in intersection_doc_ids:\n",
    "            if doc_contain_phrase(docId, phrase):     \n",
    "                result.append(docId)\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_query(query):\n",
    "    \n",
    "    exception_tokens = re.findall(r'\\!\\s(\\w+)', query)\n",
    "    phrasal_tokens = re.findall(r'\"([^\"]*)\"', query)\n",
    "    raw_tokens = re.sub(r'\\!\\s\\w+', '', query)\n",
    "    raw_tokens = re.sub(r'\"[^\"]*\"', '', raw_tokens)\n",
    "    \n",
    "    simple_tokens = preprocess_text(raw_tokens)\n",
    "    print(exception_tokens, phrasal_tokens, raw_tokens)\n",
    "    result = None\n",
    "    # sorted documents by relevance after processing simple tokens\\n\",\n",
    "    if len(simple_tokens) > 0:\n",
    "        result = simple_process(pre_processed_docs, simple_tokens)\n",
    "        \n",
    "    # removing the items that included exception tokens, kind of post filter\n",
    "    if len(exception_tokens) > 0:\n",
    "        removed_result = exceptions_process(result, exception_tokens)\n",
    "        if len(removed_result) > 0 and len(result) > 0:\n",
    "            result = [x for x in result if x in removed_result]  \n",
    "            \n",
    "    # Intersect of results with those containing phrasal queries\\n\",\n",
    "    if len(phrasal_tokens) > 0:\n",
    "        phrasal_result = phrasal_process(pre_processed_docs, phrasal_tokens)\n",
    "        \n",
    "        if result is None or len(result) == 0:\n",
    "            return phrasal_result\n",
    "        result = [x for x in result if x in phrasal_result]  \n",
    "\n",
    "    return result\n",
    "\n",
    "def query_print(query, content=False, max_cnt=20):\n",
    "    results = process_query(query)[:max_cnt]\n",
    "    if len(results) == 0:\n",
    "        print(\"Ù†ØªÛŒØ¬Ù‡ Ø§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯\")\n",
    "    for rank, doc in enumerate(results):\n",
    "        if doc == None:\n",
    "            continue\n",
    "        print(50*'=')\n",
    "        print(f'Rank: {rank + 1}, docID: {doc}')\n",
    "        for dict_ in data:\n",
    "            if dict_[\"id\"] == doc:\n",
    "            \n",
    "                print(f'From: {dict_[\"from\"]}')\n",
    "                print(f'Date: {dict_[\"date\"]}')\n",
    "                \n",
    "                print(f'{dict_[\"text\"]}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "749da4e9-5907-491a-a20e-2be3fd3e3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù…\n",
      "==================================================\n",
      "Rank: 1, docID: 4648\n",
      "From: Mahzad Sharif\n",
      "Date: 2021-07-13T11:14:39\n",
      "Ù¾Ø³ Ø§ÛŒÙ† Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… ØªØ§ Û²Û³ ØªÛŒØ± ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ù†Ù„Ø§ÛŒÙ† Ù‡Ø§ Ù‡Ø³ØªØŸÙ‡Ù†ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø§ÙØ±Ø§Ø¯ÛŒ Ú©Ù‡ Ù…ÛŒØ®ÙˆØ§Ù† Ø§ÙÙ„Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ú©Ù†Ù† Ø§Ø¹Ù„Ø§Ù… Ù†Ø´Ø¯Ù‡ Ù†Ø­ÙˆÙ‡ Ø«Ø¨Øª Ù†Ø§Ù…ØŸ\n",
      "Ù‡Ù…Ø´ Ù…ÛŒØªØ±Ø³Ù… Ø¬Ø§ Ø¨Ù…ÙˆÙ† ÙˆØ§Ù‚Ø¹Ø§ğŸ˜…ğŸ˜…ğŸ™ğŸ™\n",
      "==================================================\n",
      "Rank: 2, docID: 5613\n",
      "From: Mahzad Sharif\n",
      "Date: 2021-07-15T23:05:34\n",
      "ØªØ¹Ø¯Ø§Ø¯ Ú©Ù‡ Ø¨Ù„Ù‡  Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù†Ø¯Ø§Ø±Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ù…Ù†Ø¸ÙˆØ±Ù… Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… Ø¨ÙˆØ¯ Ø¹Ø²ÛŒØ²Ù…ğŸ™‚ğŸŒ· Ù…Ù‡Ù„Øª Ø§Ù†Ù„Ø§ÛŒÙ† ØªÙˆ Ù¾ÛŒØ§Ù… Ù¾ÛŒÙ† ØªØ§ Ø§Ù†ØªÙ‡Ø§ÛŒ Û²Û³ ØªÛŒØ± Ø¨ÙˆØ¯ ØªØ§ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ ÛŒØ§Ø¯Ù…Ù‡. Ø§ÙˆÙ† Ú©Ù‡ ØªÙ…ÙˆÙ… Ø´Ø¯Ù‡ Ø¯Ø¯Ù„Ø§ÛŒÙ†Ø´ ÙÚ© Ú©Ù†Ù…!!!\n",
      "==================================================\n",
      "Rank: 3, docID: 6226\n",
      "From: Masoud\n",
      "Date: 2021-07-16T18:21:59\n",
      "Ø¯ÙˆØ³ØªØ§Ù† Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… Ø¢ÙÙ„Ø§ÛŒÙ† Ø¨Ø§Ø² Ù…ÛŒ Ù…ÙˆÙ†Ù‡ ÛŒØ§ ØªØ§ÛŒÙ… Ù…Ø­Ø¯ÙˆØ¯ Ø¯Ø§Ø±Ù‡ Ø§ÙˆÙ†Ù…ØŸ\n",
      "==================================================\n",
      "Rank: 4, docID: 14133\n",
      "From: Huny\n",
      "Date: 2021-12-20T09:47:23\n",
      "['ğŸ’ ØªÙ†Ù‡Ø§ Ø³Ù‡ Ø±ÙˆØ² ØªØ§ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø¯ÙˆÙ…ÛŒÙ† Ø¯ÙˆØ±Ù‡ ', {'type': 'bold', 'text': 'Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¨ÛŒÙ†\\u200cØ§Ù„Ù…Ù„Ù„ÛŒ ReACT'}, ' Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª.\\n\\nğŸ’« Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¨Ø§ Ù…Ø­ÙˆØ±ÛŒØª Ø¨Ø±Ø±Ø³ÛŒ ÙÙ†Ø§ÙˆØ±ÛŒ\\u200cÙ‡Ø§ÛŒ Ù†ÙˆÛŒÙ† Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø¨Ø±Ù‚ØŒ Ø§Ù…Ø³Ø§Ù„ Ù‡Ù… Ø§ÙØªØ®Ø§Ø± Ù…ÛŒØ²Ø¨Ø§Ù†ÛŒ Ø§Ø² ÙØ¹Ø§Ù„Ø§Ù† Ø¹Ù„Ù…ÛŒ Ùˆ ØµÙ†Ø¹ØªÛŒ Ø´Ù†Ø§Ø®ØªÙ‡\\u200cØ´Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡\\u200cÙ‡Ø§ Ùˆ Ø´Ø±Ú©Øª\\u200cÙ‡Ø§ÛŒ Ù†Ø§Ù…\\u200cØ¢Ø´Ù†Ø§ÛŒ Ø³Ø±Ø§Ø³Ø± Ø¯Ù†ÛŒØ§ Ø±Ø§ Ø¯Ø§Ø±Ø¯.\\n\\nâ­ï¸ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø§Ù…Ø³Ø§Ù„ Ø¨Ù‡ Ù…Ø¯Øª Ú†Ù‡Ø§Ø± Ø±ÙˆØ² Ø¨Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…Ø¯Ø¹ÙˆÛŒÙ† Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ù…Ø¬Ø§Ø²ÛŒ ØªÙˆØ³Ø· Ø±Ø³Ø§Ù†Ø§ØŒ Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ø¨Ø±Ù‚ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙØŒ Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒ\\u200cØ´ÙˆØ¯. \\n\\nğŸ—“ Ù…Ù‡Ù„Øª Ø«Ø¨Øª\\u200cÙ†Ø§Ù… ØªØ§ ', {'type': 'underline', 'text': 'Û²Û¹'}, ' Ø¢Ø°Ø±Ù…Ø§Ù‡ Ø¯Ø± ', {'type': 'text_link', 'text': 'ÙˆØ¨\\u200cØ³Ø§ÛŒØª Ø±Ø³Ù…ÛŒ Ø±ÙˆÛŒØ¯Ø§Ø¯\\n', 'href': 'https://react.ee.sharif.edu/registration.html'}, 'ğŸ“… Ø²Ù…Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ: Û±ØŒ Û²ØŒ Û¸ Ùˆ Û¹ Ø¯ÛŒ\\u200cÙ…Ø§Ù‡.\\n\\nğŸ”» Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆÛŒØ¯Ø§Ø¯ØŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…Ø¯Ø¹ÙˆÛŒÙ† Ù…Ø­ØªØ±Ù…ØŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø±ÙˆØ§Ø¨Ø· Ø¹Ù…ÙˆÙ…ÛŒ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù…ÛŒ\\u200cØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù„ÙŠÙ†Ú©\\u200cÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§ Ù…Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ø´ÛŒØ¯: \\n\\nğŸŒ ', {'type': 'link', 'text': 'https://react.ee.sharif.edu/'}, '\\nâ–«ï¸', {'type': 'mention', 'text': '@ReACT2021'}, '\\nğŸ”¸', {'type': 'mention', 'text': '@ReACT2021_Support'}, ' \\nğŸ”¹', {'type': 'mention', 'text': '@EEResana'}, '\\n\\n', {'type': 'hashtag', 'text': '#ReACT_2021'}, '\\n', {'type': 'hashtag', 'text': '#Webinar'}, '']\n",
      "==================================================\n",
      "Rank: 5, docID: 22070\n",
      "From: None\n",
      "Date: 2022-06-10T16:56:05\n",
      "Ø§Ù‚Ø§ÛŒ Ø­Ø¬Ø§Ø²ÛŒ ØªØ§ Ø³Ø§Ø¹Øª Ú†Ù†Ø¯ Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… Ù‡Ø³ØªØŸ\n",
      "==================================================\n",
      "Rank: 6, docID: 22441\n",
      "From: Sky\n",
      "Date: 2022-06-12T12:26:43\n",
      "Ø¨Ú†Ù‡ Ù‡Ø§ ÙÛŒÙ„ØªØ±Ø´Ú©Ù† Ú© Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒØ¯ ÛŒÚ©Ù… Ø§Ø­ØªÛŒØ§Ø· Ú©Ù†ÛŒØ¯ \n",
      "Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª ÛŒÙ‡Ùˆ Ù†ØªÙˆ Ù…ÛŒØ¨Ù†Ø¯ÛŒØ¯ Ù‚Ø¨Ù„ ÙˆÛŒ Ù¾ÛŒ Ø§Ù† ÛŒØ§ Ù‡Ø± Ø¯Ù„ÛŒÙ„ Ø¯ÛŒÚ¯. Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒØ´Ù‡ ÙˆÛŒÙ†Ø¯ÙˆØ² Ú©Ø±Ú© Ø´Ø¯Ù…ÙˆÙ†ğŸ˜€Ø¨ Ù…Ø´Ú©Ù„ Ø¨Ø®ÙˆØ±Ù‡ Ùˆ ÙˆÙ‚ØªÛŒ ÙˆÛŒ Ù¾ÛŒ Ø§Ù† Ø®Ø§Ù…ÙˆØ´Ù‡ Ù…Ø´Ú©Ù„ ÙˆØ±ÙˆØ¯ Ø¨Ø§ Ø³Ø§ÛŒØªØ§Ø±Ùˆ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯. Ø¯Ø± Ø¹Ù…Ù„ Ø¯ÛŒ Ø§Ù† Ø§Ø³ Ùˆ ÛŒ Ø³Ø±ÛŒ Ú†ÛŒØ²Ø§ÛŒ Ø¯ÛŒÚ¯ ØªÙˆØ³Ø· ÙˆÛŒ Ù¾ÛŒ Ø§Ù† Ø¨Ù‡Ù… Ø®ÙˆØ±Ø¯Ù† Ùˆ Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ø±ÛŒØ³Øª Ù†Ù…ÛŒØ´Ù‡. Ø§ÛŒÙ†Ø¬Ø§Ø³ Ú©Ù‡ ÛŒÚ©Ù… Ø¨Ø§ÛŒØ¯ Ø§Ø­ØªÛŒØ§Ø· Ø¨Ù†Ù…Ø§ÛŒÛŒØ¯. Ø§Ø² Ø·Ø±ÙÛŒ Ù‡Ù… Ù…Ø´Ú©Ù„Ø§Øª Ú©Ø´ÙˆØ± Ø¬Ø¯Ø§Ø³.\n",
      "Ú©Ø³Ø§Ù†ÛŒ Ú©Ù‡ Ù…Ø´Ú©Ù„ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ø³Ø§ÛŒØª Ø±Ùˆ Ø¯Ø§Ø±Ù†Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒÚ©Ù†Ù… Ø§Ø² ÛŒÚ© Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø¯ÛŒÚ¯ Ù…Ø«Ù„Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± ÛŒØ§ Ú¯ÙˆØ´ÛŒ Ø¯ÛŒÚ¯ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†Ù†Ø¯.\n",
      "Ø§Ø² Ø¹Ù„ÛŒ Ø¬Ø§Ù† Ù‡Ù… Ø®ÙˆØ§Ù‡Ø´Ù…Ù†Ø¯ÛŒÙ… Ú©Ù‡ Ø¬Ù‡Øª Ø«Ø¨Øª Ù†Ø§Ù… Ù…Ù‡Ù„Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø±Ùˆ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¯ÙˆØ³ØªØ§Ù† Ø¨Ø¯Ù†Ø¯ (Ø±Ø§Ù‡ Ø­Ù„ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ù‡Ù… Ø¨Ø§Ø´Ù‡ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù‡).\n",
      "==================================================\n",
      "Rank: 7, docID: 34903\n",
      "From: Erfan\n",
      "Date: 2022-10-28T15:07:15\n",
      "Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¯Ùˆ ØªØ§ Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±ØªÙˆÙ† \n",
      "Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù‡ Ù…Ù‡Ù„Øª Ø®Ø§ØµÛŒ Ù†Ø¯Ø§Ø±Ù‡ Ø¯Ø³Øª Ø®ÙˆØ¯ØªÙˆÙ†Ù‡ ØŒ Ù‡Ø± ÙˆÙ‚Øª Ù…ÛŒØ®ÙˆØ§ÛŒØ¯ Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†ÛŒØ¯ .\n",
      "Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¨Ø¹Ø¯ÛŒ Ù‡Ù… Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ø®ÙˆØ¯ØªÙˆÙ† Ø¯Ø§Ø±Ù‡ ØŒ Ø§Ú¯Ù‡ Ù…ÛŒØ®ÙˆØ§ÛŒØ¯ Ø®ÙˆØ¯ØªÙˆÙ† Ø±Ùˆ Ø¨Ø±Ø³ÙˆÙ†ÛŒØ¯ Ú©Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ† Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÛŒØ¯ Ø¨Ø§ÛŒØ¯ Ø²ÙˆØ¯ØªØ± Ø¨Ø¨ÛŒÙ†ÛŒØ¯ ØŒ ÙˆÙ„ÛŒ Ø§Ú¯Ù‡ Ù†Ù‡ Ù…ÛŒØ®ÙˆØ§ÛŒØ¯ ØµØ±ÙØ§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ù…ÛŒØªÙˆÙ†ÛŒØ¯ Ø·Ø¨Ù‚ ÙˆÙ‚ØªØªÙˆÙ† Ø¬Ù„Ø³Ø§Øª Ø±Ùˆ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø¨Ø¨ÛŒÙ†ÛŒØ¯ Ùˆ ØªÙ…Ø±ÛŒÙ† Ú©Ù†ÛŒØ¯ Ùˆ Ø³ÙˆØ§Ù„ÛŒ ÛŒØ§ Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø§Ø´ØªÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯ .\n",
      "==================================================\n",
      "Rank: 8, docID: 48346\n",
      "From: Pezhman\n",
      "Date: 2023-06-01T12:13:48\n",
      "['Ø¨Ø¨Ø®Ø´ÛŒØ¯ Û² Ø³ÙˆØ§Ù„ Ø¯Ø§Ø´ØªÙ… Ú¯ÙØªÙ† Ø´Ù…Ø§ Ø§Ø² Ø´Ù…Ø§ Ø¨Ù¾Ø±Ø³Ù… :\\nØ³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ø§ÛŒÙ†Ú©Ù‡ ØªØ®ÙÛŒÙ ÛµÛ° Ø¯Ø±ØµØ¯ÛŒ ØªØ§ Ú©ÛŒ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù‡Ø³Øª ØŸ Ù…Ù‡Ù„ØªØ´ ØªÙ…ÙˆÙ… Ù†Ù…ÛŒØ´Ù‡ ØŸ Ù…Ù† Ù…ÛŒØ®ÙˆØ§Ù… Ø¬Ù„Ø³Ø§Øª Ø§ÙˆÙ„ Ø±Ùˆ Ø¨Ø¨ÛŒÙ†Ù… Ùˆ Ø§Ú¯Ø± Ø¨Ø±Ø§Ù… Ù…Ù†Ø§Ø³Ø¨ Ø¨ÙˆØ¯ Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†Ù… .\\n\\nØ³ÙˆØ§Ù„ Ø¯ÙˆÙ… Ø§ÛŒÙ†Ú©Ù‡  Ø§ÛŒÙ† Ø±ÙˆØ¯Ù…Ù¾ Ú©Ù‡ Ø¯ÙˆØ³ØªØ§Ù† Ú¯ÙØªÙ† Ù‚Ø±Ø§Ø±Ù‡ Ù…Ù†ØªØ´Ø± Ø¨Ø´Ù‡ØŒ Ø¢ÛŒØ§ Ø¨Ù‡ Ø´Ø±ÙˆØ¹ Ø§ÛŒÙ† Ø¯ÙˆØ±Ù‡ Ù‡Ø§ Ùˆ Ù‚Ø¨Ù„ Ø§Ø² Ù¾Ø§ÛŒØ§Ù† Ù…Ù‡Ù„Øª ØªØ®ÙÛŒÙ ÛµÛ° Ø¯Ø±ØµØ¯ÛŒ  Ù…ÛŒØ±Ø³Ù‡ Ú©Ù‡ Ø¨Ø¯ÙˆÙ†ÛŒÙ… Ø¨Ù‡ Ú†Ù‡ ØªØ±ØªÛŒØ¨ÛŒ Ø¨Ø§ÛŒØ¯ Ù¾ÛŒØ´ Ø¨Ø±ÛŒÙ… ØŸ\\n\\nÙ¾ÛŒØ´Ø§Ù¾ÛŒØ´ Ø§Ø² Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø´Ù…Ø§ Ø¨Ø³ÛŒØ§Ø± Ø³Ù¾Ø§Ø³Ú¯Ø²Ø§Ø±Ù… \\nğŸ™ğŸŒ¹\\n', {'type': 'mention', 'text': '@Aliii_H93'}, '']\n",
      "==================================================\n",
      "Rank: 9, docID: 48479\n",
      "From: @$$@*\n",
      "Date: 2023-06-01T14:23:48\n",
      "Ø«Ø¨Øª Ù†Ø§Ù… ØªØ§ Ú©ÛŒ Ù…Ù‡Ù„Øª Ø¯Ø§Ø±Ù‡ ØŸ\n",
      "==================================================\n",
      "Rank: 10, docID: 48940\n",
      "From: shirin\n",
      "Date: 2023-06-03T12:38:01\n",
      "Ø³Ù„Ø§Ù… ÙˆÙ‚Øª Ø¨Ø®ÙŠØ±\n",
      "Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… Ø¯ÙˆØ±Ù‡ Ù‡Ø§ ØªØ§ Ú†Ù‡ ØªØ§Ø±ÙŠØ®ÙŠ Ù‡Ø³ØªØŸ\n",
      "==================================================\n",
      "Rank: 11, docID: 48941\n",
      "From: Khosro\n",
      "Date: 2023-06-03T12:40:54\n",
      "Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… ÙØ¹Ù„Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªÛŒ Ù†Ø¯Ø§Ø±Ù‡ ÙˆÙ„ÛŒ Ú©Ø¯ ØªØ®ÙÛŒÙ Ù‡Ø§ ØªØ§ Ø¢Ø®Ø± Ø®Ø±Ø¯Ø§Ø¯ ÙØ¹Ø§Ù„ Ù‡Ø³ØªÙ†Ø¯.\n",
      "==================================================\n",
      "Rank: 12, docID: 50945\n",
      "From: Mehran\n",
      "Date: 2023-06-24T22:05:28\n",
      "Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù… Ù‚Ø¨Ù„ Ø§Ø² ØªÙ…ÙˆÙ… Ø´Ø¯Ù† Ù…Ù‡Ù„Øª ØªØ®ÙÛŒÙ Ù¾ÙˆÙ„ Ø¨Ø±Ú¯Ø´Øª Ø¨Ø®ÙˆØ±Ù‡ Ø¨ØªÙˆÙ†Ù… Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†Ù…ğŸ¤¦â€â™‚\n",
      "==================================================\n",
      "Rank: 13, docID: 56618\n",
      "From: Maedeh\n",
      "Date: 2023-08-07T13:36:00\n",
      "['ğŸ“£ ', {'type': 'bold', 'text': 'Ú©ÙˆØ¦Ø±Ø§ Ùˆ Ø§ÛŒØ±Ø§Ù†Ø³Ù„\\u200cÙ„Ø¨Ø² Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒ\\u200cÚ©Ù†Ù†Ø¯:'}, '\\n\\nğŸŸ¡ ', {'type': 'bold', 'text': 'Ù‡Ú©Ø§ØªÙˆÙ† Ù‡ÙˆØ´\\u200cÙ…ØµÙ†ÙˆØ¹ÛŒ'}, ' ', {'type': 'bold', 'text': 'Ø§ÛŒØ±Ø§Ù†Ø³Ù„\\u200cÙ„Ø¨Ø² '}, 'ğŸŸ¡\\n\\nğŸ”¸ ÙØ±ØµØªÛŒ Ø¬Ø°Ø§Ø¨ Ø¨Ø±Ø§ÛŒ', {'type': 'bold', 'text': ' '}, 'Ø¹Ù„Ø§Ù‚Ù‡\\u200cÙ…Ù†Ø¯Ø§Ù† Ø­ÙˆØ²Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡', {'type': 'bold', 'text': '\\n\\n'}, 'âœ”ï¸ Û³ Ø±ÙˆØ² Ø¢Ù…ÙˆØ²Ø´\\n\\nâœ”ï¸ Ø±Ù‚Ø§Ø¨Øª ØªÛŒÙ…ÛŒ Ø¯Ø± Ù…Ø³Ø§Ø¨Ù‚Ù‡ \\n\\nâœ”ï¸ ÛµÛ° Ù…ÛŒÙ„ÛŒÙˆÙ† ØªÙˆÙ…Ø§Ù† Ø¬Ø§ÛŒØ²Ù‡ Ù†Ù‚Ø¯ÛŒ\\n\\nâœ”ï¸ Ø§Ù‡Ø¯Ø§ÛŒ Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡ \\n\\nâœ”ï¸ Ø§Ø¹Ø·Ø§ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ù†Ø®Ø¨Ú¯ÛŒ\\n\\nâœ”ï¸ ÙØ±ØµØª Ø§Ø³ØªØ®Ø¯Ø§Ù…\\n\\n\\nâ³ ', {'type': 'bold', 'text': 'Ù…Ù‡Ù„Øª Ø«Ø¨Øª\\u200cÙ†Ø§Ù…: Û²Û³ Ù…Ø±Ø¯Ø§Ø¯'}, '\\nğŸ”» Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ùˆ Ø«Ø¨Øª\\u200cÙ†Ø§Ù…:ğŸ‘‡ğŸ‘‡\\n\\nğŸ”— ', {'type': 'link', 'text': 'https://quera.org/r/w4kk4'}, '\\nâ–â–â–â–â–â–\\n', {'type': 'hashtag', 'text': '#Quera'}, '']\n",
      "==================================================\n",
      "Rank: 14, docID: 63879\n",
      "From: AM Iravani\n",
      "Date: 2023-11-21T14:27:55\n",
      "[{'type': 'bold', 'text': 'Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ù…Ø¯Ù„ Ù‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø²Ø¨Ø§Ù†ÛŒ'}, '\\n\\nØ³Ø®Ù†Ø±Ø§Ù†Ø§Ù†:\\n', {'type': 'text_link', 'text': 'Ø¯Ú©ØªØ± Ø­Ø³ÛŒÙ† Ø§Ø®Ù„Ø§Ù‚Ù¾ÙˆØ±\\n', 'href': 'http://linkedin.com/in/hosseinakhlaghpour/'}, 'Ù…Ù‡Ù†Ø¯Ø³ Ø§Ø±Ø´Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†\\n', {'type': 'text_link', 'text': 'Ø¯Ú©ØªØ± Ø¨Ø§Ø¨Ú© Ø­Ø³ÛŒÙ† Ø®Ù„Ø¬', 'href': 'https://sharif.edu/~khalaj/'}, '\\nØ±Ø¦ÛŒØ³ Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø¨Ø±Ù‚ Ø´Ø±ÛŒÙ\\n\\n Ø¬Ù„Ø³Ù‡ Ø¨Ù‡ Ø¯Ùˆ ØµÙˆØ±Øª ', {'type': 'underline', 'text': 'Ø­Ø¶ÙˆØ±ÛŒ'}, ' Ùˆ ', {'type': 'underline', 'text': 'Ù…Ø¬Ø§Ø²ÛŒ'}, ' Ù…ÛŒ\\u200cØ¨Ø§Ø´Ø¯.\\n\\n Ø³Ù‡ Ø´Ù†Ø¨Ù‡ØŒ Û³Û° Ø¢Ø¨Ø§Ù†\\nØ³Ø§Ø¹Øª Û±Û¶ Ø§Ù„ÛŒ Û±Û¸:Û³Û°\\n\\n Ø¬Ù‡Øª Ø´Ø±Ú©Øª ', {'type': 'bold', 'text': 'Ø±Ø§ÛŒÚ¯Ø§Ù†'}, ' Ø¯Ø± Ø¬Ù„Ø³Ù‡ØŒ ', {'type': 'text_link', 'text': 'Ø§ÛŒÙ† ÙØ±Ù…', 'href': 'https://docs.google.com/forms/d/e/1FAIpQLScagOPTO6ckYFuOaBJBI8B2s4xopFz8g3vdeKnp2-eBBWZwHg/viewform?usp=sf_link'}, ' Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ ÙØ±Ù…Ø§ÛŒÛŒØ¯.\\n\\n Ø´Ø±Ú©Øª Ø¯Ø± Ø¬Ù„Ø³Ù‡ Ù‡Ù…Ø§ÛŒØ´ØŒ Ø¨Ø±Ø§ÛŒ Ø¹Ù…ÙˆÙ… Ø¢Ø²Ø§Ø¯ Ø§Ø³Øª.\\n\\nÙ…', {'type': 'bold', 'text': 'Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù… Ø¬Ù‡Øª Ø´Ø±Ú©Øª Ø­Ø¶ÙˆØ±ÛŒØŒ Ø¯ÙˆØ´Ù†Ø¨Ù‡ Û²Û¹ Ø¢Ø¨Ø§Ù† ØŒ Û±Û²:Û°Û° Ø´Ø¨\\n\\n'}, 'ØªØ§Ø±ÛŒØ® Ùˆ Ù…Ù‡Ù„Øª Ø«Ø¨Øª\\u200cÙ†Ø§Ù… ÙˆØ±Ú©Ø´Ø§Ù¾\\u200cÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ø¯ÙˆØ±Ù‡ Ù†ÛŒØ² Ø¨Ù‡ Ø²ÙˆØ¯ÛŒ Ø§Ø¹Ù„Ø§Ù… Ù…ÛŒ\\u200cØ´ÙˆØ¯.', {'type': 'bold', 'text': '\\n'}, '\\nÙ†Ø´Ø§Ù†ÛŒ Ù…Ø­Ù„ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ: ', {'type': 'text_link', 'text': 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙØŒ Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø¨Ø±Ù‚ØŒ Ø³Ø§Ù„Ù† Ú©Ù‡Ø±Ø¨Ø§\\n\\n', 'href': 'https://www.google.com/maps/dir/35.7500075,51.4774258/%D8%A8%D8%B1%D9%82+%D8%B4%D8%B1%DB%8C%D9%81%E2%80%AD%E2%80%AD/@35.73116,51.3412031,12z/data=!3m1!4b1!4m9!4m8!1m1!4e1!1m5!1m1!1s0x3f8e00a67178d75d:0xe942e83b91cc17d5!2m2!1d51.3517591!2d35.7014735'}, ' ', {'type': 'text_link', 'text': 'Ù„ÛŒÙ†Ú© Ø´Ø±Ú©Øª Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¬Ø§Ø²ÛŒ', 'href': 'https://vc.sharif.edu/ch/ee-stu'}, {'type': 'text_link', 'text': '\\n\\n', 'href': 'https://vc.sharif.edu/ch/jaryan'}, ' ', {'type': 'text_link', 'text': 'Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ ØªÙ‚ÙˆÛŒÙ…', 'href': 'https://www.addevent.com/event/kt19337055+google'}, '\\n\\nã€°ï¸ã€°ï¸ã€°ï¸ã€°ï¸ã€°ï¸\\nØ§ÛŒÙ† Ú©Ø§Ù†Ø§Ù„ Ø¨Ø§ Ù‡Ø¯Ù Ø¢Ú¯Ø§Ù‡ Ø³Ø§Ø²ÛŒ Ø§Ø² Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†Ø¸ÛŒØ± Ù‡Ù…Ø§ÛŒØ´ØŒ Ú©Ù†ÙØ±Ø§Ù†Ø³ØŒ ÙˆØ±Ú©\\u200cØ´Ø§Ù¾ Ùˆ Ú©Ù„Ø§Ø³\\xa0 ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª.\\n\\n', {'type': 'mention', 'text': '@eventai'}, '']\n",
      "==================================================\n",
      "Rank: 15, docID: 4784\n",
      "From: Nashmil\n",
      "Date: 2021-07-13T18:12:40\n",
      "[{'type': 'bold', 'text': 'ğŸ“Œ ÙØ±Ø§Ø®ÙˆØ§Ù† ØµÙ†Ø¯ÙˆÙ‚ Ù†ÙˆØ¢ÙˆØ±ÛŒ Ùˆ Ø´Ú©ÙˆÙØ§ÛŒÛŒ: \\nÚ¯Ø±Ù†Øª ØªØ­Ù‚ÛŒÙ‚ Ùˆ ØªÙˆØ³Ø¹Ù‡'}, ' \\n\\nâ¬…ï¸ ', {'type': 'bold', 'text': 'Ø­ÙˆØ²Ù‡ ÙÙ†Ø§ÙˆØ±ÛŒ: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ'}, 'â¡ï¸\\n\\nâœ³ï¸ Ù…Ù‡Ù„Øª Ø§Ø±Ø³Ø§Ù„ Ø·Ø±Ø­\\u200cÙ‡Ø§: Û³Û° ØªÛŒØ±Ù…Ø§Ù‡ Û±Û´Û°Û°\\n\\nâœ³ï¸ Ø§Ø±Ø³Ø§Ù„ Ø·Ø±Ø­\\u200cÙ‡Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ ÙˆØ§ØªØ³\\u200cØ§Ù¾ Ø´Ù…Ø§Ø±Ù‡:\\nâ˜ï¸09219572627\\n\\n- - - - - - - -\\n\\nÚ©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù…ÛŒ Ù…Ø¤Ø³Ø³Ù‡ Ù†ÛŒÙ…Ø§Ø¯ Ø§Ø·Ù„Ø§Ø¹\\u200cØ±Ø³Ø§Ù†ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†\\u200cÙ‡Ø§ÛŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø³Ù„Ø§Ù…Øª/ Ø³ÛŒØ§Ø³ØªÚ¯Ø°Ø§Ø±ÛŒ Ø³Ù„Ø§Ù…Øª Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ ØªÙˆØ³Ø¹Ù‡ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙØ±ØµØª\\u200cÙ‡Ø§ÛŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø³Ù„Ø§Ù…Øª Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒ\\u200cØ¯Ù‡Ø¯.\\n\\nğŸ†” ', {'type': 'mention', 'text': '@NIMADNews'}, '\\n- - - - - - - - -']\n",
      "==================================================\n",
      "Rank: 16, docID: 47090\n",
      "From: F Moslemi\n",
      "Date: 2023-05-20T12:07:39\n",
      "Ø³Ù„Ø§Ù… Ø¯ÙˆØ³ØªØ§Ù† Ù…Ù† ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø¨Ù† Ú©ØªØ§Ø¨ Ø¯Ø§Ø±Ù… Ø¨Ø§ Ù†ØµÙ Ù‚ÛŒÙ…Øª \n",
      "Ø§Ù…Ø±ÙˆØ² Ø¢Ø®Ø±ÛŒÙ† Ù…Ù‡Ù„Øª Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù…Ø¬Ø§Ø²ÛŒÙ‡ \n",
      "Ø§Ú¯Ø± Ú©Ø³ÛŒ Ø®ÙˆØ§Ø³Øª Ø¨Ù‡Ù… Ø§Ø·Ù„Ø§Ø¹ Ø¨Ø¯Ù‡\n",
      "==================================================\n",
      "Rank: 17, docID: 48403\n",
      "From: Sky\n",
      "Date: 2023-06-01T12:47:39\n",
      "Ø¯ÙˆÙ†Ù‡ Ø¯ÙˆÙ†Ù‡ Ø§Ø² Ø¨Ø§Ù„Ø§ Ø¬ÙˆØ§Ø¨ Ù…ÛŒØ¯Ù… ÛŒÚ©Ù… Ù…Ù‡Ù„Øª Ø¨Ø¯Ù‡ ğŸ˜€\n",
      "==================================================\n",
      "Rank: 18, docID: 48480\n",
      "From: Matin Sajadi\n",
      "Date: 2023-06-01T14:25:09\n",
      "Ø§Ø±Ù‡ Ø³ÙˆØ§Ù„ Ù…Ù†Ù… Ù‡Ø³Øª Ú©Ù„ Ú©ÙˆØ±Ø³ Ù‡Ø§ ØªØ§ Ú©ÛŒ Ù…Ù‡Ù„Øª Ø¯Ø§Ø±Ù‡ Ø«Ø¨Øª Ù†Ø§Ù…Ø´ÙˆÙ†ØŸØŸ\n",
      "==================================================\n",
      "Rank: 19, docID: 49217\n",
      "From: Raheleh mohseni\n",
      "Date: 2023-06-05T14:05:14\n",
      "[{'type': 'mention', 'text': '@Aliii_H93'}, ' ', {'type': 'mention', 'text': '@Linux_kali2'}, ' \\nØ³Ù„Ø§Ù… ÙˆÙ‚ØªØªÙˆÙ† Ø¨Ø®ÛŒØ±.\\nÙ…Ù‡Ù„Øª ØªØ®ÙÛŒÙ ÛµÛ°Ø¯Ø±ØµØ¯ÛŒ Ø¯ÙˆØ±Ù‡ Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø¨Ú†Ù‡ Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† ØªØ§ Ú©ÛŒ Ù‡Ø³ØªØŸ!']\n",
      "==================================================\n",
      "Rank: 20, docID: 50060\n",
      "From: Pezhman\n",
      "Date: 2023-06-13T11:18:04\n",
      "Ø³Ù„Ø§Ù…\n",
      "\n",
      "Ø¨Ø¨Ø®Ø´ÛŒØ¯ Ù…ÛŒØ´Ù‡ Ù…Ù‡Ù„Øª ØªØ®ÙÛŒÙ ÛµÛ° Ø¯Ø±ØµØ¯ Ø±Ùˆ ØªÙ…Ø¯ÛŒØ¯ Ú©Ù†ÛŒØ¯ ØŸ ÛŒØ¨Ø§Ø± Ú¯ÙØªÛŒÙ† ØªØ§ Ø§Ø®Ø± Ø®Ø±Ø¯Ø§Ø¯ Ù‡Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "query_print(\"Ù…Ù‡Ù„Øª Ø«Ø¨Øª Ù†Ø§Ù…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21ae9b-eca8-4a88-a55c-fdedeca45480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da719f82-5fa3-470a-9f26-f6afecf9d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cc9b6f-c816-4138-952f-5f156e23d76b",
   "metadata": {},
   "source": [
    "# Production Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0f6f1-1258-4351-b80d-5080ca9ed8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1bb1bf1-3d52-45fb-9086-ed94c541139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, stopwords_file):\n",
    "        self.normalizer = Normalizer()\n",
    "        self.stop_words = self.load_stopwords(stopwords_file)\n",
    "        \n",
    "    def load_stopwords(self, file_path):\n",
    "        logger.info(f\"Loading stopwords from {file_path}\")\n",
    "        with open(file_path) as f:\n",
    "            return {self.normalizer.normalize(line.strip()) for line in f}\n",
    "        \n",
    "    def remove_stopwords(self, text):\n",
    "        tokens = word_tokenize(self.normalizer.normalize(text))\n",
    "        return ' '.join(token for token in tokens if token not in self.stop_words)\n",
    "    \n",
    "    def de_emojify(self, text):\n",
    "        # Remove certain Unicode characters and replace emojis\n",
    "        pattern = re.compile(r\"[\\u2069\\u2066]+\", re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        return demoji.replace(text, \" \")\n",
    "    \n",
    "    def preprocess_text(self, message):\n",
    "        text_content = ''\n",
    "        if isinstance(message, list):\n",
    "            for sub_message in message:\n",
    "                if isinstance(sub_message, str):\n",
    "                    text_content += f\" {self.remove_stopwords(sub_message)}\"\n",
    "                elif isinstance(sub_message, dict) and sub_message.get('type') in {\n",
    "                    'text_link', 'bold', 'italic', 'hashtag', 'mention', 'pre'\n",
    "                }:\n",
    "                    text_content += f\" {self.remove_stopwords(sub_message['text'])}\"\n",
    "        else:\n",
    "            text_content += f\" {self.remove_stopwords(message)}\"\n",
    "        \n",
    "        tokens = word_tokenize(self.normalizer.normalize(text_content))\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess(self, input_docs):\n",
    "        processed_docs = copy.deepcopy(input_docs)\n",
    "        for doc in processed_docs:\n",
    "            if 'text' in doc and doc['text']:\n",
    "                doc['text'] = self.preprocess_text(doc['text'])\n",
    "        return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf1a706f-fc07-4fc5-a008-b389cd1f515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('6661636c9aff4f67958981a6'),\n",
       " 'id': 2,\n",
       " 'type': 'message',\n",
       " 'date': '2021-07-05T23:22:36',\n",
       " 'date_unixtime': '1625514756',\n",
       " 'from': 'a.20',\n",
       " 'from_id': 'user1278735911',\n",
       " 'text': ['Ø¢Ù‚Ø§ÛŒ Ø­Ø¬Ø§Ø²ÛŒ \\nØ§Ú¯Ø± Ø§Ø­ÛŒØ§Ù†Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†ÙØ±Ø§Øª Ù…ØªÙ‚Ø§Ø¶ÛŒ Ø´Ø±Ú©Øª Ø¯Ø± Ø¯ÙˆØ±Ù‡ Ø¨ÛŒØ´ Ø§Ø² Ø¸Ø±ÙÛŒØª Ù¾Ù„ØªÙØ±Ù… Ù‡Ø§ÛŒ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø¯ÙˆØ±Ù‡ Ø¨ÙˆØ¯ØŒ Ø®ÙˆØ§Ù‡Ø´Ø§Ù‹ Ø¯ÙˆØ±Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¢ÙÙ„Ø§ÛŒÙ† Ø¨Ø±Ú¯Ø²Ø§Ø± Ú©Ù†ÛŒØ¯\\n\\nÙ…Ø§ Ù†Ù…ÛŒ\\u200cØ®ÙˆØ§Ù‡ÛŒÙ… Ø¯ÙˆØ±Ù‡ Ø±Ùˆ  Ø§Ø² Ø¯Ø³Øª Ø¨Ø¯ÛŒÙ… \\n',\n",
       "  {'type': 'mention', 'text': '@Aliii_H93'},\n",
       "  ''],\n",
       " 'text_entities': [{'type': 'plain',\n",
       "   'text': 'Ø¢Ù‚Ø§ÛŒ Ø­Ø¬Ø§Ø²ÛŒ \\nØ§Ú¯Ø± Ø§Ø­ÛŒØ§Ù†Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†ÙØ±Ø§Øª Ù…ØªÙ‚Ø§Ø¶ÛŒ Ø´Ø±Ú©Øª Ø¯Ø± Ø¯ÙˆØ±Ù‡ Ø¨ÛŒØ´ Ø§Ø² Ø¸Ø±ÙÛŒØª Ù¾Ù„ØªÙØ±Ù… Ù‡Ø§ÛŒ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø¯ÙˆØ±Ù‡ Ø¨ÙˆØ¯ØŒ Ø®ÙˆØ§Ù‡Ø´Ø§Ù‹ Ø¯ÙˆØ±Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¢ÙÙ„Ø§ÛŒÙ† Ø¨Ø±Ú¯Ø²Ø§Ø± Ú©Ù†ÛŒØ¯\\n\\nÙ…Ø§ Ù†Ù…ÛŒ\\u200cØ®ÙˆØ§Ù‡ÛŒÙ… Ø¯ÙˆØ±Ù‡ Ø±Ùˆ  Ø§Ø² Ø¯Ø³Øª Ø¨Ø¯ÛŒÙ… \\n'},\n",
       "  {'type': 'mention', 'text': '@Aliii_H93'},\n",
       "  {'type': 'plain', 'text': ''}]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c92c1ee6-f3ab-42c3-ba37-f62aa4c185b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-10 16:05:43.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_stopwords\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading stopwords from stopwords.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "preprocessor = TextPreprocessor('stopwords.txt')\n",
    "p_data = preprocessor.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3f52d686-9d6f-4043-a59a-a3c0bb8f0f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('6661636c9aff4f67958981aa'),\n",
       "  'id': 6,\n",
       "  'type': 'message',\n",
       "  'date': '2021-07-05T23:26:28',\n",
       "  'date_unixtime': '1625514988',\n",
       "  'from': 'Sima',\n",
       "  'from_id': 'user622339736',\n",
       "  'reply_to_message_id': 2,\n",
       "  'text': 'Ù…ÙˆØ§ÙÙ‚Ù…\\nÙ…Ù† Ø­ØªÛŒ Ù†ØªÙˆÙ†Ø³ØªÙ… Ø¯Ø§Ø®Ù„ Ú©Ù„Ø§Ø³ Ø¬ÙˆÛŒÙ† Ø¨Ø´Ù… Ø¢ÙÙ„Ø§ÛŒÙ† Ù‡Ù… Ø¨Ø±Ø§Ù… Ø¨Ø§Ø² Ù†Ø´Ø¯',\n",
       "  'text_entities': [{'type': 'plain',\n",
       "    'text': 'Ù…ÙˆØ§ÙÙ‚Ù…\\nÙ…Ù† Ø­ØªÛŒ Ù†ØªÙˆÙ†Ø³ØªÙ… Ø¯Ø§Ø®Ù„ Ú©Ù„Ø§Ø³ Ø¬ÙˆÛŒÙ† Ø¨Ø´Ù… Ø¢ÙÙ„Ø§ÛŒÙ† Ù‡Ù… Ø¨Ø±Ø§Ù… Ø¨Ø§Ø² Ù†Ø´Ø¯'}]},\n",
       " {'_id': ObjectId('6661636c9aff4f67958981ab'),\n",
       "  'id': 9,\n",
       "  'type': 'message',\n",
       "  'date': '2021-07-05T23:26:37',\n",
       "  'date_unixtime': '1625514997',\n",
       "  'from': 'Sobhan Razyani',\n",
       "  'from_id': 'user72383515',\n",
       "  'reply_to_message_id': 4,\n",
       "  'text': 'Ù…Ù…Ù†ÙˆÙ†Ù…. JadyØŸ',\n",
       "  'text_entities': [{'type': 'plain', 'text': 'Ù…Ù…Ù†ÙˆÙ†Ù…. JadyØŸ'}]}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "50eb2a93-3523-4e1e-8a13-e8a4e021d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# class DocumentProcessor:\n",
    "#     def __init__(self, index, data, pre_processed_docs):\n",
    "#         self.index = index\n",
    "#         self.data = data\n",
    "#         self.pre_processed_docs = pre_processed_docs\n",
    "\n",
    "#     @staticmethod\n",
    "#     def create_index_dict(docs):\n",
    "#         index = {}\n",
    "#         for doc in docs:\n",
    "#             for pos, token in enumerate(doc['text'].split()):  # Split text into tokens\n",
    "#                 if token not in index:\n",
    "#                     index[token] = {\n",
    "#                         'total_freq': 1,\n",
    "#                         'docs': {\n",
    "#                             doc['id']: {\n",
    "#                                 'count_of_word': 1,\n",
    "#                                 'positions': [pos]\n",
    "#                             }\n",
    "#                         }\n",
    "#                     }\n",
    "#                 else:\n",
    "#                     index[token]['total_freq'] += 1\n",
    "#                     if doc['id'] not in index[token]['docs']:\n",
    "#                         index[token]['docs'][doc['id']] = {\n",
    "#                             'count_of_word': 1,\n",
    "#                             'positions': [pos]\n",
    "#                         }\n",
    "#                     else:\n",
    "#                         index[token]['docs'][doc['id']]['count_of_word'] += 1\n",
    "#                         index[token]['docs'][doc['id']]['positions'].append(pos)\n",
    "\n",
    "#         # Create champion lists\n",
    "#         for word in index:\n",
    "#             champ_list = sorted(index[word]['docs'], key=lambda x: index[word]['docs'][x]['count_of_word'], reverse=True)\n",
    "#             index[word]['champions'] = champ_list[:len(champ_list) // 5]\n",
    "#         return index\n",
    "\n",
    "#     def simple_process(self, tokens):\n",
    "#         docs_score = {}\n",
    "#         for token in tokens:\n",
    "#             if token in self.index:\n",
    "#                 for doc_id in self.index[token]['docs']:\n",
    "#                     if doc_id not in docs_score:\n",
    "#                         docs_score[doc_id] = 1\n",
    "#                     else:\n",
    "#                         docs_score[doc_id] += 1\n",
    "\n",
    "#         docs_score = sorted(docs_score.items(), key=lambda doc_score: doc_score[1], reverse=True)\n",
    "#         result = [x for x, _ in docs_score]\n",
    "#         return result\n",
    "\n",
    "#     def exceptions_process(self, docs, tokens):\n",
    "#         removal_docs = []\n",
    "#         for token in tokens:\n",
    "#             rmv_doc_ids = self.index[token]['docs'].keys()\n",
    "#             for doc_id in rmv_doc_ids:\n",
    "#                 if doc_id not in removal_docs:\n",
    "#                     removal_docs.append(doc_id)\n",
    "\n",
    "#         result = [doc_id for doc_id in docs if doc_id not in removal_docs]\n",
    "#         return result\n",
    "\n",
    "#     def doc_contain_phrase(self, doc_id, phrase):\n",
    "#         tokens_lst = phrase.split()\n",
    "#         positions = []\n",
    "#         for token in tokens_lst:\n",
    "#             positions.append(list(self.index[token]['docs'][doc_id]['positions']))\n",
    "\n",
    "#         for i in range(len(positions[0])):\n",
    "#             flag = True\n",
    "#             index_of_first_token = positions[0][i]\n",
    "#             for j in range(len(positions)):\n",
    "#                 if index_of_first_token + j not in positions[j]:\n",
    "#                     flag = False\n",
    "#             if flag:\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     def phrasal_process(self, phrases):\n",
    "#         posting_lists = []\n",
    "#         result = []\n",
    "#         for phrase in phrases:\n",
    "#             words = phrase.split()\n",
    "#             for word in words:\n",
    "#                 if word in self.index:\n",
    "#                     posting_lists.append(list(self.index[word]['docs'].keys()))\n",
    "\n",
    "#             intersection_doc_ids = set.intersection(*map(set, posting_lists))\n",
    "\n",
    "#             for doc_id in intersection_doc_ids:\n",
    "#                 if self.doc_contain_phrase(doc_id, phrase):\n",
    "#                     result.append(doc_id)\n",
    "#         return result\n",
    "\n",
    "#     def process_query(self, query):\n",
    "#         exception_tokens = re.findall(r'\\!\\s(\\w+)', query)\n",
    "#         phrasal_tokens = re.findall(r'\"([^\"]*)\"', query)\n",
    "#         raw_tokens = re.sub(r'\\!\\s\\w+', '', query)\n",
    "#         raw_tokens = re.sub(r'\"[^\"]*\"', '', raw_tokens)\n",
    "#         simple_tokens = self.preprocess_text(raw_tokens)\n",
    "\n",
    "#         result = None\n",
    "#         if simple_tokens:\n",
    "#             result = self.simple_process(simple_tokens)\n",
    "\n",
    "#         if exception_tokens:\n",
    "#             removed_result = self.exceptions_process(result, exception_tokens)\n",
    "#             if removed_result and result:\n",
    "#                 result = [x for x in result if x in removed_result]\n",
    "\n",
    "#         if phrasal_tokens:\n",
    "#             phrasal_result = self.phrasal_process(phrasal_tokens)\n",
    "#             if result is None or not result:\n",
    "#                 return phrasal_result\n",
    "#             result = [x for x in result if x in phrasal_result]\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     def query_print(self, query, content=False, max_cnt=20):\n",
    "#         results = self.process_query(query)[:max_cnt]\n",
    "#         if not results:\n",
    "#             print(\"Ù†ØªÛŒØ¬Ù‡ Ø§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯\")\n",
    "#         for rank, doc in enumerate(results):\n",
    "#             if doc is None:\n",
    "#                 continue\n",
    "#             print(50 * '=')\n",
    "#             print(f'Rank: {rank + 1}, docID: {doc}')\n",
    "#             for dict_ in self.data:\n",
    "#                 if dict_[\"id\"] == doc:\n",
    "#                     print(f'From: {dict_[\"from\"]}')\n",
    "#                     print(f'Date: {dict_[\"date\"]}')\n",
    "#                     print(f'{dict_[\"text\"]}')\n",
    "#                     break\n",
    "\n",
    "#     def preprocess_text(self, text):\n",
    "#         # Implement your text preprocessing here\n",
    "#         return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "467297fa-36a3-41e8-87f8-e32c44b400e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example\n",
    "# index = {\n",
    "#     'example': {\n",
    "#         'docs': {\n",
    "#             1: {'positions': [1, 2]},\n",
    "#             2: {'positions': [3]}\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# data = [\n",
    "#     {\"id\": 1, \"from\": \"User1\", \"date\": \"2023-01-01\", \"text\": \"Example text 1\"},\n",
    "#     {\"id\": 2, \"from\": \"User2\", \"date\": \"2023-01-02\", \"text\": \"Example text 2\"}\n",
    "# ]\n",
    "# pre_processed_docs = [1, 2]\n",
    "\n",
    "# processor = DocumentProcessor(index, data, pre_processed_docs)\n",
    "# processor.query_print('example query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722f4c3-a802-452c-9c2d-a3813362d827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
