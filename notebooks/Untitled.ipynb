{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb82ba52-3df5-443f-9bf6-0ebb04b95d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from hazm import Normalizer, word_tokenize, stopwords_list, Stemmer\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import arabic_reshaper\n",
    "import demoji\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from bidi.algorithm import get_display\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2202f3a-fcb4-40da-bd1a-977c8a17a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"pytopia\"\n",
    "COLLECTION_NAME = \"messages\"\n",
    "START_DATE = '2022-07-01T00:00:00'\n",
    "END_DATE = '2023-07-31T23:59:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dedce3eb-124a-46b2-ad79-515603e58abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client[DB_NAME]\n",
    "cursor = db[COLLECTION_NAME].find({})\n",
    "# for document in cursor:\n",
    "#     pprint(document)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72748df2-7dac-4f36-91f4-25f45b596dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data = [doc for doc in cursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ea7897-dd14-4061-8e33-b34f5cc56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4154f8f3-228e-4633-ba5b-8fa7eba0bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-10 16:05:24.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mLoading stopwords from stopwords.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# load stopwords\n",
    "logger.info(f\"Loading stopwords from {'stopwords.txt'}\")\n",
    "stop_words = open('stopwords.txt').readlines()\n",
    "stop_words = map(str.strip, stop_words)\n",
    "stop_words = set(map(normalizer.normalize, stop_words))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(normalizer.normalize(text))\n",
    "    tokens = list(filter(lambda item: item not in stop_words, tokens))\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def de_emojify(text):\n",
    "    regrex_pattern = re.compile(pattern=\"[\\u2069\\u2066]+\", flags=re.UNICODE)\n",
    "    text = regrex_pattern.sub('', text)\n",
    "    return demoji.replace(text, \" \")\n",
    "\n",
    "def preprocess_text(msg):\n",
    "    text_content = ''\n",
    "    if isinstance(msg, list):\n",
    "        for sub_msg in msg:\n",
    "            if isinstance(sub_msg, str):\n",
    "                text_content += f\" {remove_stopwords(sub_msg)}\"\n",
    "            elif isinstance(sub_msg, dict) and sub_msg['type'] in {\n",
    "                'text_link', 'bold', 'italic',\n",
    "                'hashtag', 'mention', 'pre'\n",
    "            }:\n",
    "                text_ = remove_stopwords(sub_msg['text'])\n",
    "                text_content += f\" {text_}\"\n",
    "    else:\n",
    "        text_content += f\" {remove_stopwords(msg)}\"\n",
    "\n",
    "    tokens = list(word_tokenize(normalizer.normalize(text_content)))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33f05cee-191b-4acb-bc08-b020eed9118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_docs):\n",
    "    docs = copy.deepcopy(input_docs)\n",
    "    for doc in docs:\n",
    "        if not doc.get('text'):\n",
    "            continue\n",
    "        content = doc['text']\n",
    "        doc['text'] = preprocess_text(content)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85e79e30-c4c0-4f3d-a221-0cf2d5c066f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🛑بچه ها یه نکته که البته خیلی هم مهم نیست \\nشما وقتی به صورت manually   فایل رو از این لینک دانلود می کنید\\n',\n",
       " {'type': 'link',\n",
       "  'text': 'https://docs.microsoft.com/en-us/windows/wsl/install-manual'},\n",
       " '\\nطبیعتا باید با یک فایل تک با پسوند appx رو به رو باشید\\nحالا بعضی بچه ها که با IDM دانلود می کنن IDM پسوند فایل رو به ZIP تغییر میده  و بچه ها هم اون فایل رو باز می کنن و سردرگم  میشن\\nکه مساله خاصی نیست خودتون دوباره پسوند فایل رو دستی از zip بکنید appx  و wsl اتون که الان یه تک فایل هست رو نصب کنید']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[180][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdcbd5cb-a600-46ab-90c5-3c201f534a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "527fe6cb-1466-4de0-8ef4-653029f40c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('6661636c9aff4f6795898336'),\n",
       " 'id': 1041,\n",
       " 'type': 'message',\n",
       " 'date': '2021-07-06T12:35:21',\n",
       " 'date_unixtime': '1625562321',\n",
       " 'from': 'MEHDI',\n",
       " 'from_id': 'user310785297',\n",
       " 'reply_to_message_id': 629,\n",
       " 'text': ['اره',\n",
       "  'میدونم',\n",
       "  'یهو',\n",
       "  'انقلابی',\n",
       "  'تصمیم',\n",
       "  'گرفتم',\n",
       "  'سوییچ',\n",
       "  'ابونتو',\n",
       "  '😂'],\n",
       " 'text_entities': [{'type': 'plain',\n",
       "   'text': 'اره میدونم میشه ولی من یهو خیلی انقلابی تصمیم گرفتم سوییچ کنم به ابونتو 😂'}]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a13c6423-6b80-488e-8a35-82602bdb42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_dict(docs):\n",
    "    \n",
    "    index = {}\n",
    "    for doc in docs:    \n",
    "        for pos, token in enumerate(doc['text']):\n",
    "            if token not in index:\n",
    "                index[token] = {\n",
    "                'total_freq': 1,\n",
    "                 'docs': {\n",
    "                       doc['id']: {\n",
    "                           'count_of_word': 1,\n",
    "                           'positions': [pos]\n",
    "                           }\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                index[token]['total_freq'] += 1  \n",
    "                if doc['id'] not in index[token]['docs']:\n",
    "                    index[token]['docs'][doc['id']] = {}\n",
    "                    index[token]['docs'][doc['id']]['count_of_word'] = 1\n",
    "                    index[token]['docs'][doc['id']]['positions'] = [pos]\n",
    "                else:\n",
    "                    index[token]['docs'][doc['id']]['count_of_word'] += 1 \n",
    "                    index[token]['docs'][doc['id']]['positions'].append(pos)   \n",
    "    \n",
    "    # create champion lists             \n",
    "    for word in index:\n",
    "        champ_list = sorted(index[word]['docs'], key=lambda x: index[word]['docs'][x]['count_of_word'], reverse=True)\n",
    "        index[word]['champions'] = champ_list[:len(champ_list) // 5]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2da43dd-e562-46bd-b49c-4f7b5c1f8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_docs = p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a99152e0-7b8e-4a71-8007-0ee03f0d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index_dict(p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d8a48aa-2e23-4768-b2a4-42f831f08004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_process(docs, tokens):\n",
    "    docs_score = {}\n",
    "    for token in tokens:\n",
    "        if token in index:\n",
    "            for doc_id in index[token]['docs']:\n",
    "                    if doc_id not in docs_score.keys():\n",
    "                        docs_score[doc_id] = 1\n",
    "                    else:\n",
    "                        docs_score[doc_id] += 1\n",
    "   \n",
    "    docs_score = sorted(docs_score.items(), key=lambda doc_score: doc_score[1], reverse=True)\n",
    "    result = [x for x,_ in docs_score]\n",
    "    return result\n",
    "\n",
    "\n",
    "def exceptions_process(docs, tokens):\n",
    "    removal_docs = []\n",
    "    for token in tokens:\n",
    "        rmv_docIds = index[token]['docs'].keys()\n",
    "        for doc_id in rmv_docIds:  \n",
    "             if removal_docs.count(doc_id) == 0:\n",
    "                    removal_docs.append(doc_id)\n",
    "\n",
    "    result = []            \n",
    "    for doc_id in docs:\n",
    "        if removal_docs.count(doc_id) == 0:\n",
    "            result.append(doc_id)\n",
    "\n",
    "    return result\n",
    "\n",
    "def doc_contain_phrase(docId, phrase):\n",
    "   # we need just a sequence of numbers in positions to find a doc contain special phrase\",\n",
    "    tokens_lst = phrase.split()\n",
    "    positions = []\n",
    "    for token in tokens_lst:\n",
    "        positions.append(list(index[token]['docs'][docId]['positions']))\n",
    "\n",
    "    for i in range (len(positions[0])):\n",
    "        flag = True\n",
    "        index_of_first_token = positions[0][i]\n",
    "        for j in range (len(positions)):\n",
    "            if positions[j].count(index_of_first_token + j) == 0:\n",
    "                flag = False\n",
    "        if flag :\n",
    "            return True\n",
    "    return flag\n",
    "\n",
    "def phrasal_process(docs, phrases):\n",
    "    posting_lists = []\n",
    "    result = []\n",
    "    for phrase in phrases:\n",
    "        words = phrase.split()\n",
    "        for word in words:\n",
    "            if word in index:\n",
    "                posting_lists.append(list(index[word]['docs'].keys()))\n",
    "    \n",
    "        intersection_doc_ids = set.intersection(*map(set, posting_lists))\n",
    "\n",
    "        for docId in intersection_doc_ids:\n",
    "            if doc_contain_phrase(docId, phrase):     \n",
    "                result.append(docId)\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_query(query):\n",
    "    \n",
    "    exception_tokens = re.findall(r'\\!\\s(\\w+)', query)\n",
    "    phrasal_tokens = re.findall(r'\"([^\"]*)\"', query)\n",
    "    raw_tokens = re.sub(r'\\!\\s\\w+', '', query)\n",
    "    raw_tokens = re.sub(r'\"[^\"]*\"', '', raw_tokens)\n",
    "    \n",
    "    simple_tokens = preprocess_text(raw_tokens)\n",
    "    print(exception_tokens, phrasal_tokens, raw_tokens)\n",
    "    result = None\n",
    "    # sorted documents by relevance after processing simple tokens\\n\",\n",
    "    if len(simple_tokens) > 0:\n",
    "        result = simple_process(pre_processed_docs, simple_tokens)\n",
    "        \n",
    "    # removing the items that included exception tokens, kind of post filter\n",
    "    if len(exception_tokens) > 0:\n",
    "        removed_result = exceptions_process(result, exception_tokens)\n",
    "        if len(removed_result) > 0 and len(result) > 0:\n",
    "            result = [x for x in result if x in removed_result]  \n",
    "            \n",
    "    # Intersect of results with those containing phrasal queries\\n\",\n",
    "    if len(phrasal_tokens) > 0:\n",
    "        phrasal_result = phrasal_process(pre_processed_docs, phrasal_tokens)\n",
    "        \n",
    "        if result is None or len(result) == 0:\n",
    "            return phrasal_result\n",
    "        result = [x for x in result if x in phrasal_result]  \n",
    "\n",
    "    return result\n",
    "\n",
    "def query_print(query, content=False, max_cnt=20):\n",
    "    results = process_query(query)[:max_cnt]\n",
    "    if len(results) == 0:\n",
    "        print(\"نتیجه ای یافت نشد\")\n",
    "    for rank, doc in enumerate(results):\n",
    "        if doc == None:\n",
    "            continue\n",
    "        print(50*'=')\n",
    "        print(f'Rank: {rank + 1}, docID: {doc}')\n",
    "        for dict_ in data:\n",
    "            if dict_[\"id\"] == doc:\n",
    "            \n",
    "                print(f'From: {dict_[\"from\"]}')\n",
    "                print(f'Date: {dict_[\"date\"]}')\n",
    "                \n",
    "                print(f'{dict_[\"text\"]}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "749da4e9-5907-491a-a20e-2be3fd3e3a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] مهلت ثبت نام\n",
      "==================================================\n",
      "Rank: 1, docID: 4648\n",
      "From: Mahzad Sharif\n",
      "Date: 2021-07-13T11:14:39\n",
      "پس این مهلت ثبت نام تا ۲۳ تیر فقط برای انلاین ها هست؟هنوز برای افرادی که میخوان افلاین شرکت کنن اعلام نشده نحوه ثبت نام؟\n",
      "همش میترسم جا بمون واقعا😅😅🙏🙏\n",
      "==================================================\n",
      "Rank: 2, docID: 5613\n",
      "From: Mahzad Sharif\n",
      "Date: 2021-07-15T23:05:34\n",
      "تعداد که بله  محدودیت نداره احتمالا منظورم مهلت ثبت نام بود عزیزم🙂🌷 مهلت انلاین تو پیام پین تا انتهای ۲۳ تیر بود تا جایی که یادمه. اون که تموم شده ددلاینش فک کنم!!!\n",
      "==================================================\n",
      "Rank: 3, docID: 6226\n",
      "From: Masoud\n",
      "Date: 2021-07-16T18:21:59\n",
      "دوستان مهلت ثبت نام آفلاین باز می مونه یا تایم محدود داره اونم؟\n",
      "==================================================\n",
      "Rank: 4, docID: 14133\n",
      "From: Huny\n",
      "Date: 2021-12-20T09:47:23\n",
      "['💎 تنها سه روز تا برگزاری دومین دوره ', {'type': 'bold', 'text': 'رویداد بین\\u200cالمللی ReACT'}, ' باقی مانده است.\\n\\n💫 این رویداد با محوریت بررسی فناوری\\u200cهای نوین مهندسی برق، امسال هم افتخار میزبانی از فعالان علمی و صنعتی شناخته\\u200cشده از دانشگاه\\u200cها و شرکت\\u200cهای نام\\u200cآشنای سراسر دنیا را دارد.\\n\\n⭐️ رویداد امسال به مدت چهار روز با ارائه مدعوین در قالب مجازی توسط رسانا، انجمن علمی دانشکده برق دانشگاه صنعتی شریف، برگزار می\\u200cشود. \\n\\n🗓 مهلت ثبت\\u200cنام تا ', {'type': 'underline', 'text': '۲۹'}, ' آذرماه در ', {'type': 'text_link', 'text': 'وب\\u200cسایت رسمی رویداد\\n', 'href': 'https://react.ee.sharif.edu/registration.html'}, '📅 زمان برگزاری: ۱، ۲، ۸ و ۹ دی\\u200cماه.\\n\\n🔻 برای دریافت اخبار رویداد، مشاهده خلاصه ارائه مدعوین محترم، مشاهده برنامه زمانی رویداد و همچنین ارتباط با روابط عمومی رویداد می\\u200cتوانید از طریق لينک\\u200cهای زیر با ما در ارتباط باشید: \\n\\n🌐 ', {'type': 'link', 'text': 'https://react.ee.sharif.edu/'}, '\\n▫️', {'type': 'mention', 'text': '@ReACT2021'}, '\\n🔸', {'type': 'mention', 'text': '@ReACT2021_Support'}, ' \\n🔹', {'type': 'mention', 'text': '@EEResana'}, '\\n\\n', {'type': 'hashtag', 'text': '#ReACT_2021'}, '\\n', {'type': 'hashtag', 'text': '#Webinar'}, '']\n",
      "==================================================\n",
      "Rank: 5, docID: 22070\n",
      "From: None\n",
      "Date: 2022-06-10T16:56:05\n",
      "اقای حجازی تا ساعت چند مهلت ثبت نام هست؟\n",
      "==================================================\n",
      "Rank: 6, docID: 22441\n",
      "From: Sky\n",
      "Date: 2022-06-12T12:26:43\n",
      "بچه ها فیلترشکن ک استفاده میکنید یکم احتیاط کنید \n",
      "گاهی اوقات یهو نتو میبندید قبل وی پی ان یا هر دلیل دیگ. این باعث میشه ویندوز کرک شدمون😀ب مشکل بخوره و وقتی وی پی ان خاموشه مشکل ورود با سایتارو داشته باشید. در عمل دی ان اس و ی سری چیزای دیگ توسط وی پی ان بهم خوردن و گاهی اوقات ریست نمیشه. اینجاس که یکم باید احتیاط بنمایید. از طرفی هم مشکلات کشور جداس.\n",
      "کسانی که مشکل ورود به سایت رو دارند پیشنهاد میکنم از یک دستگاه دیگ مثلا کامپیوتر یا گوشی دیگ امتحان کنند.\n",
      "از علی جان هم خواهشمندیم که جهت ثبت نام مهلت بیشتری رو در اختیار دوستان بدند (راه حل مناسبی هم باشه خیلی خوبه).\n",
      "==================================================\n",
      "Rank: 7, docID: 34903\n",
      "From: Erfan\n",
      "Date: 2022-10-28T15:07:15\n",
      "در مورد دو تا سوال آخرتون \n",
      "ثبت نام که مهلت خاصی نداره دست خودتونه ، هر وقت میخواید ثبت نام کنید .\n",
      "در مورد بعدی هم بستگی به خودتون داره ، اگه میخواید خودتون رو برسونید که آنلاین ادامه بدید باید زودتر ببینید ، ولی اگه نه میخواید صرفا یاد بگیرید میتونید طبق وقتتون جلسات رو به ترتیب ببینید و تمرین کنید و سوالی یا مشکلی داشتید اینجا بپرسید .\n",
      "==================================================\n",
      "Rank: 8, docID: 48346\n",
      "From: Pezhman\n",
      "Date: 2023-06-01T12:13:48\n",
      "['ببخشید ۲ سوال داشتم گفتن شما از شما بپرسم :\\nسوال اول اینکه تخفیف ۵۰ درصدی تا کی برقرار هست ؟ مهلتش تموم نمیشه ؟ من میخوام جلسات اول رو ببینم و اگر برام مناسب بود ثبت نام کنم .\\n\\nسوال دوم اینکه  این رودمپ که دوستان گفتن قراره منتشر بشه، آیا به شروع این دوره ها و قبل از پایان مهلت تخفیف ۵۰ درصدی  میرسه که بدونیم به چه ترتیبی باید پیش بریم ؟\\n\\nپیشاپیش از راهنمایی شما بسیار سپاسگزارم \\n🙏🌹\\n', {'type': 'mention', 'text': '@Aliii_H93'}, '']\n",
      "==================================================\n",
      "Rank: 9, docID: 48479\n",
      "From: @$$@*\n",
      "Date: 2023-06-01T14:23:48\n",
      "ثبت نام تا کی مهلت داره ؟\n",
      "==================================================\n",
      "Rank: 10, docID: 48940\n",
      "From: shirin\n",
      "Date: 2023-06-03T12:38:01\n",
      "سلام وقت بخير\n",
      "مهلت ثبت نام دوره ها تا چه تاريخي هست؟\n",
      "==================================================\n",
      "Rank: 11, docID: 48941\n",
      "From: Khosro\n",
      "Date: 2023-06-03T12:40:54\n",
      "مهلت ثبت نام فعلا محدودیتی نداره ولی کد تخفیف ها تا آخر خرداد فعال هستند.\n",
      "==================================================\n",
      "Rank: 12, docID: 50945\n",
      "From: Mehran\n",
      "Date: 2023-06-24T22:05:28\n",
      "امیدوارم قبل از تموم شدن مهلت تخفیف پول برگشت بخوره بتونم ثبت نام کنم🤦‍♂\n",
      "==================================================\n",
      "Rank: 13, docID: 56618\n",
      "From: Maedeh\n",
      "Date: 2023-08-07T13:36:00\n",
      "['📣 ', {'type': 'bold', 'text': 'کوئرا و ایرانسل\\u200cلبز برگزار می\\u200cکنند:'}, '\\n\\n🟡 ', {'type': 'bold', 'text': 'هکاتون هوش\\u200cمصنوعی'}, ' ', {'type': 'bold', 'text': 'ایرانسل\\u200cلبز '}, '🟡\\n\\n🔸 فرصتی جذاب برای', {'type': 'bold', 'text': ' '}, 'علاقه\\u200cمندان حوزه هوش مصنوعی، یادگیری ماشین و تحلیل داده', {'type': 'bold', 'text': '\\n\\n'}, '✔️ ۳ روز آموزش\\n\\n✔️ رقابت تیمی در مسابقه \\n\\n✔️ ۵۰ میلیون تومان جایزه نقدی\\n\\n✔️ اهدای گواهینامه \\n\\n✔️ اعطای امتیاز نخبگی\\n\\n✔️ فرصت استخدام\\n\\n\\n⏳ ', {'type': 'bold', 'text': 'مهلت ثبت\\u200cنام: ۲۳ مرداد'}, '\\n🔻 اطلاعات بیشتر و ثبت\\u200cنام:👇👇\\n\\n🔗 ', {'type': 'link', 'text': 'https://quera.org/r/w4kk4'}, '\\n➖➖➖➖➖➖\\n', {'type': 'hashtag', 'text': '#Quera'}, '']\n",
      "==================================================\n",
      "Rank: 14, docID: 63879\n",
      "From: AM Iravani\n",
      "Date: 2023-11-21T14:27:55\n",
      "[{'type': 'bold', 'text': 'رمزگشایی مدل های بزرگ زبانی'}, '\\n\\nسخنرانان:\\n', {'type': 'text_link', 'text': 'دکتر حسین اخلاقپور\\n', 'href': 'http://linkedin.com/in/hosseinakhlaghpour/'}, 'مهندس ارشد یادگیری ماشین\\n', {'type': 'text_link', 'text': 'دکتر بابک حسین خلج', 'href': 'https://sharif.edu/~khalaj/'}, '\\nرئیس دانشکده مهندسی برق شریف\\n\\n جلسه به دو صورت ', {'type': 'underline', 'text': 'حضوری'}, ' و ', {'type': 'underline', 'text': 'مجازی'}, ' می\\u200cباشد.\\n\\n سه شنبه، ۳۰ آبان\\nساعت ۱۶ الی ۱۸:۳۰\\n\\n جهت شرکت ', {'type': 'bold', 'text': 'رایگان'}, ' در جلسه، ', {'type': 'text_link', 'text': 'این فرم', 'href': 'https://docs.google.com/forms/d/e/1FAIpQLScagOPTO6ckYFuOaBJBI8B2s4xopFz8g3vdeKnp2-eBBWZwHg/viewform?usp=sf_link'}, ' را تکمیل فرمایید.\\n\\n شرکت در جلسه همایش، برای عموم آزاد است.\\n\\nم', {'type': 'bold', 'text': 'هلت ثبت نام جهت شرکت حضوری، دوشنبه ۲۹ آبان ، ۱۲:۰۰ شب\\n\\n'}, 'تاریخ و مهلت ثبت\\u200cنام ورکشاپ\\u200cهای این دوره نیز به زودی اعلام می\\u200cشود.', {'type': 'bold', 'text': '\\n'}, '\\nنشانی محل برگزاری: ', {'type': 'text_link', 'text': 'دانشگاه صنعتی شریف، دانشکده مهندسی برق، سالن کهربا\\n\\n', 'href': 'https://www.google.com/maps/dir/35.7500075,51.4774258/%D8%A8%D8%B1%D9%82+%D8%B4%D8%B1%DB%8C%D9%81%E2%80%AD%E2%80%AD/@35.73116,51.3412031,12z/data=!3m1!4b1!4m9!4m8!1m1!4e1!1m5!1m1!1s0x3f8e00a67178d75d:0xe942e83b91cc17d5!2m2!1d51.3517591!2d35.7014735'}, ' ', {'type': 'text_link', 'text': 'لینک شرکت به صورت مجازی', 'href': 'https://vc.sharif.edu/ch/ee-stu'}, {'type': 'text_link', 'text': '\\n\\n', 'href': 'https://vc.sharif.edu/ch/jaryan'}, ' ', {'type': 'text_link', 'text': 'اضافه کردن به تقویم', 'href': 'https://www.addevent.com/event/kt19337055+google'}, '\\n\\n〰️〰️〰️〰️〰️\\nاین کانال با هدف آگاه سازی از رویدادهای مرتبط با هوش مصنوعی نظیر همایش، کنفرانس، ورک\\u200cشاپ و کلاس\\xa0 تشکیل شده است.\\n\\n', {'type': 'mention', 'text': '@eventai'}, '']\n",
      "==================================================\n",
      "Rank: 15, docID: 4784\n",
      "From: Nashmil\n",
      "Date: 2021-07-13T18:12:40\n",
      "[{'type': 'bold', 'text': '📌 فراخوان صندوق نوآوری و شکوفایی: \\nگرنت تحقیق و توسعه'}, ' \\n\\n⬅️ ', {'type': 'bold', 'text': 'حوزه فناوری: هوش مصنوعی'}, '➡️\\n\\n✳️ مهلت ارسال طرح\\u200cها: ۳۰ تیرماه ۱۴۰۰\\n\\n✳️ ارسال طرح\\u200cها از طریق واتس\\u200cاپ شماره:\\n☎️09219572627\\n\\n- - - - - - - -\\n\\nکانال تلگرامی مؤسسه نیماد اطلاع\\u200cرسانی فراخوان\\u200cهای حمایت از پژوهش و فناوری در حوزه سلامت/ سیاستگذاری سلامت و رویدادهای مرتبط را در راستای توسعه دسترسی به فرصت\\u200cهای حمایت از پژوهش و فناوری سلامت انجام می\\u200cدهد.\\n\\n🆔 ', {'type': 'mention', 'text': '@NIMADNews'}, '\\n- - - - - - - - -']\n",
      "==================================================\n",
      "Rank: 16, docID: 47090\n",
      "From: F Moslemi\n",
      "Date: 2023-05-20T12:07:39\n",
      "سلام دوستان من تعدادی بن کتاب دارم با نصف قیمت \n",
      "امروز آخرین مهلت نمایشگاه مجازیه \n",
      "اگر کسی خواست بهم اطلاع بده\n",
      "==================================================\n",
      "Rank: 17, docID: 48403\n",
      "From: Sky\n",
      "Date: 2023-06-01T12:47:39\n",
      "دونه دونه از بالا جواب میدم یکم مهلت بده 😀\n",
      "==================================================\n",
      "Rank: 18, docID: 48480\n",
      "From: Matin Sajadi\n",
      "Date: 2023-06-01T14:25:09\n",
      "اره سوال منم هست کل کورس ها تا کی مهلت داره ثبت نامشون؟؟\n",
      "==================================================\n",
      "Rank: 19, docID: 49217\n",
      "From: Raheleh mohseni\n",
      "Date: 2023-06-05T14:05:14\n",
      "[{'type': 'mention', 'text': '@Aliii_H93'}, ' ', {'type': 'mention', 'text': '@Linux_kali2'}, ' \\nسلام وقتتون بخیر.\\nمهلت تخفیف ۵۰درصدی دوره پایتون برای بچه های ایران تا کی هست؟!']\n",
      "==================================================\n",
      "Rank: 20, docID: 50060\n",
      "From: Pezhman\n",
      "Date: 2023-06-13T11:18:04\n",
      "سلام\n",
      "\n",
      "ببخشید میشه مهلت تخفیف ۵۰ درصد رو تمدید کنید ؟ یبار گفتین تا اخر خرداد هست\n"
     ]
    }
   ],
   "source": [
    "query_print(\"مهلت ثبت نام\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21ae9b-eca8-4a88-a55c-fdedeca45480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da719f82-5fa3-470a-9f26-f6afecf9d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cc9b6f-c816-4138-952f-5f156e23d76b",
   "metadata": {},
   "source": [
    "# Production Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0f6f1-1258-4351-b80d-5080ca9ed8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1bb1bf1-3d52-45fb-9086-ed94c541139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, stopwords_file):\n",
    "        self.normalizer = Normalizer()\n",
    "        self.stop_words = self.load_stopwords(stopwords_file)\n",
    "        \n",
    "    def load_stopwords(self, file_path):\n",
    "        logger.info(f\"Loading stopwords from {file_path}\")\n",
    "        with open(file_path) as f:\n",
    "            return {self.normalizer.normalize(line.strip()) for line in f}\n",
    "        \n",
    "    def remove_stopwords(self, text):\n",
    "        tokens = word_tokenize(self.normalizer.normalize(text))\n",
    "        return ' '.join(token for token in tokens if token not in self.stop_words)\n",
    "    \n",
    "    def de_emojify(self, text):\n",
    "        # Remove certain Unicode characters and replace emojis\n",
    "        pattern = re.compile(r\"[\\u2069\\u2066]+\", re.UNICODE)\n",
    "        text = pattern.sub('', text)\n",
    "        return demoji.replace(text, \" \")\n",
    "    \n",
    "    def preprocess_text(self, message):\n",
    "        text_content = ''\n",
    "        if isinstance(message, list):\n",
    "            for sub_message in message:\n",
    "                if isinstance(sub_message, str):\n",
    "                    text_content += f\" {self.remove_stopwords(sub_message)}\"\n",
    "                elif isinstance(sub_message, dict) and sub_message.get('type') in {\n",
    "                    'text_link', 'bold', 'italic', 'hashtag', 'mention', 'pre'\n",
    "                }:\n",
    "                    text_content += f\" {self.remove_stopwords(sub_message['text'])}\"\n",
    "        else:\n",
    "            text_content += f\" {self.remove_stopwords(message)}\"\n",
    "        \n",
    "        tokens = word_tokenize(self.normalizer.normalize(text_content))\n",
    "        return tokens\n",
    "    \n",
    "    def preprocess(self, input_docs):\n",
    "        processed_docs = copy.deepcopy(input_docs)\n",
    "        for doc in processed_docs:\n",
    "            if 'text' in doc and doc['text']:\n",
    "                doc['text'] = self.preprocess_text(doc['text'])\n",
    "        return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf1a706f-fc07-4fc5-a008-b389cd1f515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('6661636c9aff4f67958981a6'),\n",
       " 'id': 2,\n",
       " 'type': 'message',\n",
       " 'date': '2021-07-05T23:22:36',\n",
       " 'date_unixtime': '1625514756',\n",
       " 'from': 'a.20',\n",
       " 'from_id': 'user1278735911',\n",
       " 'text': ['آقای حجازی \\nاگر احیانا تعداد نفرات متقاضی شرکت در دوره بیش از ظرفیت پلتفرم های برگزاری دوره بود، خواهشاً دوره را به صورت آفلاین برگزار کنید\\n\\nما نمی\\u200cخواهیم دوره رو  از دست بدیم \\n',\n",
       "  {'type': 'mention', 'text': '@Aliii_H93'},\n",
       "  ''],\n",
       " 'text_entities': [{'type': 'plain',\n",
       "   'text': 'آقای حجازی \\nاگر احیانا تعداد نفرات متقاضی شرکت در دوره بیش از ظرفیت پلتفرم های برگزاری دوره بود، خواهشاً دوره را به صورت آفلاین برگزار کنید\\n\\nما نمی\\u200cخواهیم دوره رو  از دست بدیم \\n'},\n",
       "  {'type': 'mention', 'text': '@Aliii_H93'},\n",
       "  {'type': 'plain', 'text': ''}]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c92c1ee6-f3ab-42c3-ba37-f62aa4c185b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-10 16:05:43.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_stopwords\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoading stopwords from stopwords.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "preprocessor = TextPreprocessor('stopwords.txt')\n",
    "p_data = preprocessor.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3f52d686-9d6f-4043-a59a-a3c0bb8f0f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('6661636c9aff4f67958981aa'),\n",
       "  'id': 6,\n",
       "  'type': 'message',\n",
       "  'date': '2021-07-05T23:26:28',\n",
       "  'date_unixtime': '1625514988',\n",
       "  'from': 'Sima',\n",
       "  'from_id': 'user622339736',\n",
       "  'reply_to_message_id': 2,\n",
       "  'text': 'موافقم\\nمن حتی نتونستم داخل کلاس جوین بشم آفلاین هم برام باز نشد',\n",
       "  'text_entities': [{'type': 'plain',\n",
       "    'text': 'موافقم\\nمن حتی نتونستم داخل کلاس جوین بشم آفلاین هم برام باز نشد'}]},\n",
       " {'_id': ObjectId('6661636c9aff4f67958981ab'),\n",
       "  'id': 9,\n",
       "  'type': 'message',\n",
       "  'date': '2021-07-05T23:26:37',\n",
       "  'date_unixtime': '1625514997',\n",
       "  'from': 'Sobhan Razyani',\n",
       "  'from_id': 'user72383515',\n",
       "  'reply_to_message_id': 4,\n",
       "  'text': 'ممنونم. Jady؟',\n",
       "  'text_entities': [{'type': 'plain', 'text': 'ممنونم. Jady؟'}]}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "50eb2a93-3523-4e1e-8a13-e8a4e021d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# class DocumentProcessor:\n",
    "#     def __init__(self, index, data, pre_processed_docs):\n",
    "#         self.index = index\n",
    "#         self.data = data\n",
    "#         self.pre_processed_docs = pre_processed_docs\n",
    "\n",
    "#     @staticmethod\n",
    "#     def create_index_dict(docs):\n",
    "#         index = {}\n",
    "#         for doc in docs:\n",
    "#             for pos, token in enumerate(doc['text'].split()):  # Split text into tokens\n",
    "#                 if token not in index:\n",
    "#                     index[token] = {\n",
    "#                         'total_freq': 1,\n",
    "#                         'docs': {\n",
    "#                             doc['id']: {\n",
    "#                                 'count_of_word': 1,\n",
    "#                                 'positions': [pos]\n",
    "#                             }\n",
    "#                         }\n",
    "#                     }\n",
    "#                 else:\n",
    "#                     index[token]['total_freq'] += 1\n",
    "#                     if doc['id'] not in index[token]['docs']:\n",
    "#                         index[token]['docs'][doc['id']] = {\n",
    "#                             'count_of_word': 1,\n",
    "#                             'positions': [pos]\n",
    "#                         }\n",
    "#                     else:\n",
    "#                         index[token]['docs'][doc['id']]['count_of_word'] += 1\n",
    "#                         index[token]['docs'][doc['id']]['positions'].append(pos)\n",
    "\n",
    "#         # Create champion lists\n",
    "#         for word in index:\n",
    "#             champ_list = sorted(index[word]['docs'], key=lambda x: index[word]['docs'][x]['count_of_word'], reverse=True)\n",
    "#             index[word]['champions'] = champ_list[:len(champ_list) // 5]\n",
    "#         return index\n",
    "\n",
    "#     def simple_process(self, tokens):\n",
    "#         docs_score = {}\n",
    "#         for token in tokens:\n",
    "#             if token in self.index:\n",
    "#                 for doc_id in self.index[token]['docs']:\n",
    "#                     if doc_id not in docs_score:\n",
    "#                         docs_score[doc_id] = 1\n",
    "#                     else:\n",
    "#                         docs_score[doc_id] += 1\n",
    "\n",
    "#         docs_score = sorted(docs_score.items(), key=lambda doc_score: doc_score[1], reverse=True)\n",
    "#         result = [x for x, _ in docs_score]\n",
    "#         return result\n",
    "\n",
    "#     def exceptions_process(self, docs, tokens):\n",
    "#         removal_docs = []\n",
    "#         for token in tokens:\n",
    "#             rmv_doc_ids = self.index[token]['docs'].keys()\n",
    "#             for doc_id in rmv_doc_ids:\n",
    "#                 if doc_id not in removal_docs:\n",
    "#                     removal_docs.append(doc_id)\n",
    "\n",
    "#         result = [doc_id for doc_id in docs if doc_id not in removal_docs]\n",
    "#         return result\n",
    "\n",
    "#     def doc_contain_phrase(self, doc_id, phrase):\n",
    "#         tokens_lst = phrase.split()\n",
    "#         positions = []\n",
    "#         for token in tokens_lst:\n",
    "#             positions.append(list(self.index[token]['docs'][doc_id]['positions']))\n",
    "\n",
    "#         for i in range(len(positions[0])):\n",
    "#             flag = True\n",
    "#             index_of_first_token = positions[0][i]\n",
    "#             for j in range(len(positions)):\n",
    "#                 if index_of_first_token + j not in positions[j]:\n",
    "#                     flag = False\n",
    "#             if flag:\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     def phrasal_process(self, phrases):\n",
    "#         posting_lists = []\n",
    "#         result = []\n",
    "#         for phrase in phrases:\n",
    "#             words = phrase.split()\n",
    "#             for word in words:\n",
    "#                 if word in self.index:\n",
    "#                     posting_lists.append(list(self.index[word]['docs'].keys()))\n",
    "\n",
    "#             intersection_doc_ids = set.intersection(*map(set, posting_lists))\n",
    "\n",
    "#             for doc_id in intersection_doc_ids:\n",
    "#                 if self.doc_contain_phrase(doc_id, phrase):\n",
    "#                     result.append(doc_id)\n",
    "#         return result\n",
    "\n",
    "#     def process_query(self, query):\n",
    "#         exception_tokens = re.findall(r'\\!\\s(\\w+)', query)\n",
    "#         phrasal_tokens = re.findall(r'\"([^\"]*)\"', query)\n",
    "#         raw_tokens = re.sub(r'\\!\\s\\w+', '', query)\n",
    "#         raw_tokens = re.sub(r'\"[^\"]*\"', '', raw_tokens)\n",
    "#         simple_tokens = self.preprocess_text(raw_tokens)\n",
    "\n",
    "#         result = None\n",
    "#         if simple_tokens:\n",
    "#             result = self.simple_process(simple_tokens)\n",
    "\n",
    "#         if exception_tokens:\n",
    "#             removed_result = self.exceptions_process(result, exception_tokens)\n",
    "#             if removed_result and result:\n",
    "#                 result = [x for x in result if x in removed_result]\n",
    "\n",
    "#         if phrasal_tokens:\n",
    "#             phrasal_result = self.phrasal_process(phrasal_tokens)\n",
    "#             if result is None or not result:\n",
    "#                 return phrasal_result\n",
    "#             result = [x for x in result if x in phrasal_result]\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     def query_print(self, query, content=False, max_cnt=20):\n",
    "#         results = self.process_query(query)[:max_cnt]\n",
    "#         if not results:\n",
    "#             print(\"نتیجه ای یافت نشد\")\n",
    "#         for rank, doc in enumerate(results):\n",
    "#             if doc is None:\n",
    "#                 continue\n",
    "#             print(50 * '=')\n",
    "#             print(f'Rank: {rank + 1}, docID: {doc}')\n",
    "#             for dict_ in self.data:\n",
    "#                 if dict_[\"id\"] == doc:\n",
    "#                     print(f'From: {dict_[\"from\"]}')\n",
    "#                     print(f'Date: {dict_[\"date\"]}')\n",
    "#                     print(f'{dict_[\"text\"]}')\n",
    "#                     break\n",
    "\n",
    "#     def preprocess_text(self, text):\n",
    "#         # Implement your text preprocessing here\n",
    "#         return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "467297fa-36a3-41e8-87f8-e32c44b400e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example\n",
    "# index = {\n",
    "#     'example': {\n",
    "#         'docs': {\n",
    "#             1: {'positions': [1, 2]},\n",
    "#             2: {'positions': [3]}\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# data = [\n",
    "#     {\"id\": 1, \"from\": \"User1\", \"date\": \"2023-01-01\", \"text\": \"Example text 1\"},\n",
    "#     {\"id\": 2, \"from\": \"User2\", \"date\": \"2023-01-02\", \"text\": \"Example text 2\"}\n",
    "# ]\n",
    "# pre_processed_docs = [1, 2]\n",
    "\n",
    "# processor = DocumentProcessor(index, data, pre_processed_docs)\n",
    "# processor.query_print('example query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722f4c3-a802-452c-9c2d-a3813362d827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
